# Multimodal-Image-Captioning-Benchmark
Benchmarked proprietary and open-source multimodal visionâ€“language models on image captioning using a controlled subset of the Flickr8k dataset. Built a Python-based evaluation pipeline in Google Colab with Hugging Face Transformers, analyzing semantic alignment using ROUGE-L and BERTScore to compare multimodal grounding and deployment trade-offs.
