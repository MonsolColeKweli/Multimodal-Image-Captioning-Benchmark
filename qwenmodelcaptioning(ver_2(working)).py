# -*- coding: utf-8 -*-
"""434Final-MonsolCole-Kweli(colekwm)-Part1(ver 2(working)).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sZ4CT9p500u2lxKihyLORA7-iRMgzEJ
"""

# Qwen3-VL 8B Abliterated captioning pipeline
# - Model: prithivMLmods/Qwen3-VL-8B-Abliterated-Caption-it
# - Dataset: Flickr8k (clip-benchmark/wds_flickr8k variant with fields: jpg, txt)
# - Metrics: METEOR, ROUGE-L, BERTScore
# - Output CSV schema (Option A): image_id, abliterated_caption, gt_1..gt_5, meteor, rouge_l, bert_f1, bert_precision, bert_recall

# Install dependencies (uncomment in Colab)
# !pip install git+https://github.com/QwenLM/qwen-vl.git
!pip install transformers datasets accelerate torchvision sentencepiece rouge-score bert-score nltk tqdm pillow

import csv
import random
import time

from datasets import load_dataset
from PIL import Image
from tqdm.auto import tqdm

import torch
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from bert_score import score as bertscore_score

from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
# from qwen_vl_utils import process_vision_info # --- having trouble with the git

import nltk
nltk.download("wordnet", quiet=True)
nltk.download("omw-1.4", quiet=True)

# imported code directly for qwen_vl_utils import process_vision_info
def process_vision_info(messages):
    """
    Extract images from Qwen messages format:
    [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": PIL.Image or np.array},
                ...
            ]
        }
    ]

    Returns (image_list, video_list)
    Video list will always be empty for this pipeline.
    """

    image_list = []
    video_list = []  # not used

    for msg in messages:
        if "content" not in msg:
            continue
        for item in msg["content"]:
            if item["type"] == "image":
                image_list.append(item["image"])
            # model also supports videos, we ignore them here

    return image_list, video_list

# ================================================================
# QWEN MODEL CONFIG
# ================================================================
MODEL_NAME = "prithivMLmods/Qwen3-VL-8B-Abliterated-Caption-it"
NUM_IMAGES = 20         # you can set 100 or other; started at 20 for speed/safety
RANDOM_SEED = 42
OUTPUT_CSV = "qwen_flickr8k.csv"
API_DELAY = 3.0         # seconds between model calls (reduce if you're confident)
MAX_NEW_TOKENS = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ================================================================
# QWWEN MODEL LOAD
# ================================================================
print("Loading Qwen3-VL modelâ€¦")

print(f"Loading model: {MODEL_NAME}")
print("Loading model...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "prithivMLmods/Qwen2-VL-2B-Abliterated-Caption-it", torch_dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("prithivMLmods/Qwen2-VL-2B-Abliterated-Caption-it")

# ================================================================
# LOAD DATASET
# ================================================================

print("Loading Flickr8k dataset from Hugging Face (may take a moment)...")
dataset = load_dataset("clip-benchmark/wds_flickr8k")

# The dataset may have splits; use 'test' split if available, otherwise sample across 'train'
available_splits = list(dataset.keys())
print("Available splits:", available_splits)

# Prefer 'test' split if present
if "test" in dataset:
    split = "test"
elif "validation" in dataset:
    split = "validation"
else:
    split = "train"

# test if data is retrievable and check format
print(f"Using split: {split}")
print(dataset)
print(dataset[split][0])
print("Sample item keys:", dataset[split][0].keys())
dataset = dataset[split]

# ================================================================
# SAMPLE IMAGES
# ================================================================

random.seed(RANDOM_SEED)
indices = random.sample(range(len(dataset)), NUM_IMAGES)

rows = []
candidate_list = []
refs_list = []

# ================================================================
# DATASET EXTRACTION FUNCTIONS
# ================================================================

def extract_image(example):
    """Extract the PIL image from the 'jpg' field."""
    return example["jpg"]

def extract_captions(example):
    """Split newline-separated captions into list of 5 clean strings."""
    raw = example["txt"]
    return [c.strip() for c in raw.split("\n") if c.strip()]

def extract_image_id(example, idx):
    """Use '__key__' field to identify images."""
    return example["__key__"] if "__key__" in example else f"image_{idx}"


# ================================================================
# QWEN CAPTION GENERATION
# ================================================================

def caption_with_qwen(pil_img):
    # Build Qwen-VL chat-style messages
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": pil_img},
                {"type": "text",  "text": "Describe this image in one sentence."},
            ],
        }
    ]

    # Apply the Qwen chat template
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Extract images (and videos, not used)
    image_inputs, video_inputs = process_vision_info(messages)

    # IMPORTANT FIX:
    # remove video from the processor or it will try to run video preprocessing
        inputs = processor(
            text=[text],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        )

    # Move to device
    inputs = inputs.to(DEVICE)

    # Generate caption
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=64
        )

    # Trim input tokens to get only model output (same as official example)
    generated_ids_trimmed = generated_ids[:, inputs.input_ids.shape[1]:]

    # Decode the caption
    caption = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0].strip()

    # fallback for empty model output
    if not caption:
        return "error"

    return caption

# extra sanity check if needed T_T
img = dataset[0]["jpg"]
print(img)
print(caption_with_qwen(img))

# ================================================================
# METRICS FUNCTIONS
# ================================================================

def compute_meteor(cand, refs):
    try:
        return meteor_score(refs, cand)
    except:
        return 0.0

def compute_rouge_l(cand, refs):
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    return max(scorer.score(r, cand)["rougeL"].fmeasure for r in refs)

def compute_bertscore_batch(cands, refs):
    # flatten
    flat_cand, flat_refs, idx_map = [], [], []
    for i, (c, rs) in enumerate(zip(cands, refs)):
        for r in rs:
            flat_cand.append(c)
            flat_refs.append(r)
            idx_map.append(i)

    P, R, F = bertscore_score(flat_cand, flat_refs, lang="en", rescale_with_baseline=True)
    P, R, F = P.tolist(), R.tolist(), F.tolist()

    best_p = [0]*len(cands)
    best_r = [0]*len(cands)
    best_f = [0]*len(cands)

    for p, r, f, i in zip(P, R, F, idx_map):
        if f > best_f[i]:
            best_f[i] = f
            best_p[i] = p
            best_r[i] = r

    return best_f, best_p, best_r

# ================================================================
# MAIN LOOP
# ================================================================

print("Generating captions...")
for idx in tqdm(indices):
    ex = dataset[idx]
    img = ex["jpg"]
    refs = [r.strip() for r in ex["txt"].split("\n") if r.strip()]
    refs5 = refs[:5] + [""]*(5 - len(refs))

    caption = caption_with_qwen(img)
    print(caption)

    meteor = compute_meteor(caption, refs)
    rouge = compute_rouge_l(caption, refs)

    candidate_list.append(caption)
    refs_list.append(refs)

    rows.append({
        "image_id": ex.get("__key__", f"image_{idx}"),
        "abliterated_caption": caption,
        "gt_1": refs5[0],
        "gt_2": refs5[1],
        "gt_3": refs5[2],
        "gt_4": refs5[3],
        "gt_5": refs5[4],
        "meteor": meteor,
        "rouge_l": rouge,
        "bert_f1": None,
        "bert_precision": None,
        "bert_recall": None,
    })

# ================================================================
# BERTScore Batch Processing
# ================================================================

print("Computing BERTScore...")
f1, p, r = compute_bertscore_batch(candidate_list, refs_list)
for i in range(len(rows)):
    rows[i]["bert_f1"] = f1[i]
    rows[i]["bert_precision"] = p[i]
    rows[i]["bert_recall"] = r[i]

# ================================================================
# SAVE CSV
# ================================================================

print("Saving CSV:", OUTPUT_CSV)

fieldnames = [
    "image_id","abliterated_caption",
    "gt_1","gt_2","gt_3","gt_4","gt_5",
    "meteor","rouge_l","bert_f1","bert_precision","bert_recall"
]

with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(rows)

print("DONE. Results saved to:", OUTPUT_CSV)